# https://github.com/NLPchina/ansj_seg/wiki/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E
# 可以通过多种方式加载你的词典.也可以自定义词典加载的接口.目前有如下方式
# 文件加载 file://c:/dic.txt 当然你可以依旧保持旧的写法c:/dic.txt
# 从jar包中加载 jar://org.ansj.dic.DicReader|/crf.model , 以jar://开头..第一个为类全名称,后面为这个类所在jar包词典文件的路径.
# 从jdbc包中加载 jdbc://jdbc:mysql://192.168.10.103:3306/dic_table?useUnicode=true&characterEncoding=utf-8&zeroDateTimeBehavior=convertToNull|username|password|select name as name,nature,freq from dic where type=1
# 从url中加载.url://http://maven.nlpcn.org/down/library/default.dic
# 当然你也可以实现自己的 加载方式,通过继承 PathToStream来实现.


#dic	"library/default.dic"	自定义词典路径
dic = src/test/resources/chinese/library/dic.dic
#dic_[key]	"你的词典路径"	针对不同语料调用不同的自定义词典

#ambiguity	"library/ambiguity.dic"		歧义词典路径
ambiguity = src/test/resources/chinese/library/ambiguity.dic
#ambiguity_[key]	"library/ambiguity.dic" 歧义词典路径

#synonyms	"默认的同义词典"	针对不同语料调用不同的分词模型
synonyms = src/test/resources/chinese/library/synonyms.dic
#synonyms_[key]	"你的同义词典路径"	针对不同语料调用不同的分词模型

#stop	"默认的过滤词典"	针对不同语料调用不同的分词模型
stop = src/test/resources/chinese/library/stop.dic
#stop_[key]	"你的过滤词典路径"	针对不同语料调用不同的分词模型

#crf	null	crf词典路径,不设置为默认
#crf = src/test/resources/tokenizer/library/crf.dic
#crf_[key]	"你的模型路径"	针对不同语料调用不同的分词模型

#是否取得真实的词,默认情况会取得标注化后的
isRealName = true

#是否开启人名识别
isNameRecognition = true

#是否开启数字识别
isNumRecognition = true

#是否数字和量词合并
isQuantifierRecognition = true

#是否用户辞典不加载相同的词
isSkipUserDefine = true
